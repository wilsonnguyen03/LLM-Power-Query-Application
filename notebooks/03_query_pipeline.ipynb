{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "220daba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\University\\UQ\\projects\\LLM-Power Knowledge Miner\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d411fd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3747 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Question: What is the main topic discussed in the document?\n",
      "\n",
      "ðŸ’¬ Answer: Let us lay the cornerstone of American freedom without fear. To hesitate is to perish Bolvar establishes Gran Colombia 220 Life without industry is guilt Stephensonâ€™s Rocket enters service 226 You may choose to look the other way, but you can never again say you did not know The Slave Trade Abolition Act 228 Society was cut in two The 1848 revolutions 230 This enterprise will return immense rewards The construction of the Suez Canal 236 Endless forms most beautiful and most wonderful have been and are being evolved Darwin publishes On the Origin of Species 238 Let us arm. Let us fight for our brothers The Expedition of the Thousand 242 These sad scenes of death and sorrow, when are they to come to an end? The Siege of Lucknow243 Better to abolish serfdom from above, than to wait for it to abolish itself from below Russia emancipates the serfs 244 Government\n",
      "\n",
      "ðŸ“š Top supporting contexts:\n",
      "\n",
      "- of the Bastille 214 I must make of all the peoples of Europe one people, and of Paris the capital of the world The Battle of Waterloo216 Let us lay the cornerstone of American freedom without fear. To ...\n",
      "\n",
      "- 66disease, spread of 136, 149â€“50, 151, 158, 159, 269divine right 101DNA 21, 236, 338Dollar Diplomacy 233Dollfuss, Engelbert 285Drogheda, Siege of 199Dunkirk, evacuation of 290DÃ¼rer, Albrecht 155Dutch  ...\n",
      "\n",
      "- 12, 35, 54, 57, 71, 104, 131Hanâ€™gul alphabet 130, 131Hannibal 70â€“1Harald Hardrada of Norway 95Harrison, John 189Harun al-Rashid, Caliph 88â€“90Hawkins, John 176Al-Haythem 91Hegel, Georg 14, 240Hellenist ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Load FAISS index and processed chunks\n",
    "index = faiss.read_index(\"../models/embeddings.faiss\")\n",
    "df = pd.read_csv(\"../data/processed/chunks_embedded.csv\")\n",
    "\n",
    "# 2. Load the same embedding model used before\n",
    "embed_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# 3. Load a small language model for answer generation\n",
    "# You can use a local or HuggingFace-hosted model.\n",
    "# This one runs on CPU and gives good quality short answers.\n",
    "qa_model = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "# 4. Helper: retrieve top relevant chunks\n",
    "def retrieve_context(query, k=3):\n",
    "    q_emb = embed_model.encode([query])\n",
    "    D, I = index.search(np.array(q_emb).astype(\"float32\"), k)\n",
    "    contexts = df.iloc[I[0]][\"text\"].tolist()\n",
    "    return contexts\n",
    "\n",
    "# 5. Helper: build the full prompt for the LLM\n",
    "def build_prompt(query, contexts):\n",
    "    context_text = \"\\n\\n\".join(contexts)\n",
    "    prompt = f\"Answer the question based only on the context below.\\n\\nContext:\\n{context_text}\\n\\nQuestion: {query}\\n\\nAnswer briefly:\"\n",
    "    return prompt\n",
    "\n",
    "# 6. Helper: get final answer\n",
    "def answer_query(query):\n",
    "    contexts = retrieve_context(query)\n",
    "    prompt = build_prompt(query, contexts)\n",
    "    answer = qa_model(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
    "    return answer, contexts\n",
    "\n",
    "# 7. Example query\n",
    "query = \"What is the main topic discussed in the document?\"\n",
    "answer, contexts = answer_query(query)\n",
    "\n",
    "print(\"ðŸ§  Question:\", query)\n",
    "print(\"\\nðŸ’¬ Answer:\", answer)\n",
    "print(\"\\nðŸ“š Top supporting contexts:\\n\")\n",
    "for c in contexts:\n",
    "    print(\"-\", c[:200], \"...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
